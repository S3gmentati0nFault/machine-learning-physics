\documentclass{article}

\usepackage{amsmath}
\usepackage{hyperref}

\newcommand{\gss}{\mathcal{N}}
\newcommand{\gexp}[1]{\frac{1}{\sqrt{2\pi} \sigma_{#1}} e^{\frac{(x - \mu_{#1})^2}{2\sigma_{#1}^2}}}
\newcommand{\spc}{\:\:}
\newcommand{\prob}[1]{\textbf{P}[#1]}
\newcommand{\cprob}[2]{\prob{#1 \spc | \spc #2}}

\title{Machine learning for physics \\ \large Sheet 2}
\author{Alessandro Biagiotti \& Mario Massimo}
\date{October 2023}

\begin{document}
\maketitle
\section{Exercise 1}
This first exercise is very simple, the following are the data at our disposal
\begin{equation*}
    \prob{\gamma} = 0.1 \hspace{0.5cm} 
    \prob{bgnd} = 0.9 \hspace{0.5cm}
    \cprob{target}{\gamma} = 0.95 \hspace{0.5cm}
    \cprob{target}{bgnd} = 0.1
\end{equation*}
What we want to know is simpy $\cprob{\gamma}{target}$ thus we just start from Bayes theorem, that grants us the following:
\begin{equation}
    \cprob{\gamma}{target} = \frac{\cprob{target}{\gamma} \cdot \prob{\gamma}}{\prob{target}}
\end{equation}
We do not know the probability of hitting the target direction, but we know two different conditional events that describe all of the possible cases we are interested in. Thanks to the law of total probability we can conclude the following:
\begin{equation}
    \prob{target} = \sum_n{\cprob{target}{B_n} \cdot \prob{B_n}}
\end{equation}
which in our case means that
\begin{align*}
    \prob{target} &= \cprob{target}{\gamma} \cdot \prob{\gamma} + \cprob{target}{bgnd} \cdot \prob{bgnd} \\
    &= 0.1 \cdot 0.95 + 0.9 \cdot 0.1 = \textbf{0.185}
\end{align*}
To complete the exercise:
\begin{equation*}
    \cprob{\gamma}{target} = \frac{0.95 \cdot 0.1}{0.185} \approx \textbf{0.51}
\end{equation*}
Thus the probability of identifying correctly $\gamma$-rays amidst background is around 51\%.
\pagebreak
\section{Exercise 3}
\subsection{a}
\begin{equation*}
    - log(p(data|\theta))= - \left(-\frac{log(2\pi \sigma^2)}{2} - \frac{\sum_{i=1}^N (p_i - p(V_{m,i},\theta))^2}{2 \sigma^2}\right) \propto 
   + \frac{\sum_{i=1}^N (p_i - p(V_{m,i},\theta))^2}{2 \sigma^2}
\end{equation*}

The expression for the negative log-likelihood is, out of constant, the one used in a $\chi^2$ test with the same error for every point. The minimization problem is then reduced to minimize

\begin{equation*}
    \sum_{i=1}^N (p_i - p(V_{m,i},\theta))^2
\end{equation*}
    
\end{document}